\def\year{2016}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai16}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (Compressed Conditional Mean Embeddings for Model-Based Reinforcement Learning)
/Author (Guy Lever, John Shawe-Taylor, Ronnie Stafford, Csaba Szepesvari)} %
\setcounter{secnumdepth}{2}

%%everything below here and before \begin{document} is added by the authors

%% we are allowed to "minimally" make these spacing changes

\setlength\abovecaptionskip{0.6mm}
\setlength\belowcaptionskip{0.6mm}
\setlength\abovedisplayskip{0.7mm}
\setlength\belowdisplayskip{0.7mm}
%\setlength\floatsep{0mm}
\setlength\textfloatsep{0.7mm}

%% Csaba's commands
%\usepackage[backgroundcolor = White]{todonotes}
\usepackage[disable]{todonotes}  %turn notes on/off with \usepackage[disable]{todonotes} 
% todo by Csaba
\newcommand{\todoc}[2][]{\todo[color=red!20!white,size=\tiny,#1]{#2}}
% todo by Guy
\newcommand{\todog}[2][]{\todo[inline,color=orange!20!white,size=\tiny#1]{#2}}
%\setlength{\marginparwidth}{10ex}
\usepackage{paralist}

%% Guys' commands
\usepackage{times}
\usepackage{graphicx}
%\usepackage{subfigure}
%\usepackage{caption}
\usepackage{subcaption} 
%\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage{hyperref}
\usepackage{url}
\usepackage{bm}
\usepackage{latexsym}
\usepackage{amsmath,amsfonts,amssymb,color}
\usepackage{mathrsfs}
\usepackage{pifont}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

%\newenvironment{proof}[1][Proof]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

%\newcommand{\qed}{\nobreak \ifvmode \relax \else
%      \ifdim\lastskip<1.5em \hskip-\lastskip
%      \hskip1.5em plus0em minus0.5em \fi \nobreak
%      \vrule height0.75em width0.5em depth0.25em\fi}

\newcommand{\vanHoofNonParametricControl}{DBLP:conf/aistats/Hoof0N15}
\newcommand{\CsabaFLAM}{DBLP:conf/adprl/YaoSPZ14}
\newcommand{\GrunewalderEmbeddingsRL}{GrunewalderEmbeddingsMDP}
\newcommand{\BertsekasApproximate}{BertsekasApproximate}
\newcommand{\BertsekasDynamic}{BertsekasDynamic}
\newcommand{\GrunewalderEmbeddingsRegression}{GrunewalderEmbeddingsRegression}
\newcommand{\SongNonparametric}{DBLP:journals/jmlr/SongGG10}
\newcommand{\SongKernelEmbedding}{DBLP:journals/spm/SongFG13}
\newcommand{\MichelliVectorValued}{DBLP:journals/neco/MicchelliP05}
\newcommand{\ParrLinear}{DBLP:conf/icml/ParrLTPL08}
\newcommand{\SuttonDyna}{sutton2008a}
\newcommand{\KroemerNonParametric}{DBLP:conf/nips/KroemerP11}
\newcommand{\DeisenrothPilco}{DBLP:conf/icml/DeisenrothR11}
\newcommand{\MallatMatchingPursuit}{DBLP:journals/tsp/MallatZ93}
\newcommand{\BengioKernelMP}{DBLP:journals/ml/VincentB02}
\newcommand{\ShaweTaylorBook}{DBLP:books/daglib/0026002}
\newcommand{\OrmoneitKBRL}{DBLP:journals/ml/OrmoneitS02}
\newcommand{\BarretoStochasticFactorization}{DBLP:conf/nips/BarretoPP11}
\newcommand{\KvetonRepresentativeKBRL}{DBLP:conf/aaai/KvetonT12}
\newcommand{\WatsonNadarayaWatson}{Wats:1964}
\newcommand{\NadarayaNadarayaWatson}{Nada:1964}
\newcommand{\ZhangMPConsistencey}{DBLP:journals/jmlr/Zhang09}
\newcommand{\HussainTheoryOfMP}{DBLP:conf/nips/HussainS08}
\newcommand{\BachLowRank}{DBLP:conf/icml/BachJ05}
\newcommand{\CsabaPersonalCommunication}{CsabaPersonalCommunication}
\newcommand{\BoydADMM}{DBLP:journals/ftml/BoydPCPE11}
\newcommand{\DuchiProjections}{DBLP:conf/icml/DuchiSSC08}
\newcommand{\BradtkeLSTD}{DBLP:journals/ml/BradtkeB96}
\newcommand{\BairdBRmin}{DBLP:conf/icml/Baird95}
\newcommand{\BellmanValueIt}{Bellman:1957}
\newcommand{\HowardPolicyIt}{howard:dp}
\newcommand{\LeverPoliciesInRKHS}{DBLP:conf/aistats/LeverS15}
\newcommand{\TibshiraniLasso}{tibshirani96regression}
\newcommand{\SinghEligibility}{DBLP:journals/ml/SinghS96}

\newcommand{\cD}{{\mathcal D}}
\newcommand{\cC}{{\mathcal C}}
\newcommand{\cB}{{\mathcal B}}
\newcommand{\cH}{{\mathcal H}}
\newcommand{\cF}{{\mathcal F}}
\newcommand{\cA}{{\mathcal A}}
\newcommand{\cS}{{\mathcal S}}
\newcommand{\cP}{{\mathcal P}}
\newcommand{\cV}{{\mathcal V}}
\newcommand{\cX}{{\mathcal X}}
\newcommand{\cM}{{\mathcal M}}
\newcommand{\cL}{{\mathcal L}}
\newcommand{\cE}{{\mathcal E}}
\newcommand{\cR}{{\mathcal R}}
\newcommand{\cG}{{\mathcal G}}
\newcommand{\br}{{\bm r}}
\newcommand{\bLambda}{{\bm \Lambda}}
\newcommand{\balpha}{{\bm \alpha}}
\newcommand{\bbeta}{{\bm \beta}}
\newcommand{\bZero}{{\bm 0}}
\newcommand{\bL}{{\bm L}}
\newcommand{\bV}{{\bm V}}
\newcommand{\bK}{{\bm K}}
\newcommand{\bM}{{\bm M}}
\newcommand{\bW}{{\bm W}}
\newcommand{\bR}{{\bm R}}
\newcommand{\bA}{{\bm A}}
\newcommand{\bI}{{\bm I}}
\newcommand{\bY}{{\bm Y}}
\newcommand{\bx}{{\bm x}}
\newcommand{\bF}{{\bm F}}
\newcommand{\bbb}{{\bm b}}
%\newcommand{\bB}{{\bm B}}
\newcommand{\bq}{{\bm q}}
\newcommand{\bPsi}{{\bm \Psi}}
\newcommand{\bPhi}{{\bm \Phi}}
\newcommand{\bw}{{\bm w}}
\newcommand{\E}{{\mathbb E}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\Lin}{{\rm Lin}}
\newcommand{\loss}{\operatornamewithlimits{loss}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\proj}{\operatornamewithlimits{proj}}
\newcommand{\greedy}{\operatornamewithlimits{greedy}}
\newcommand{\trace}{\operatornamewithlimits{trace}}
\newcommand{\lang}{\langle}
\newcommand{\rang}{\rangle}
\newcommand{\nn}{\nonumber}
\newcommand{\reg}{\operatornamewithlimits{reg}}
\newcommand{\bzero}{{\bm 0}}
\newcommand{\bone}{{\bm 1}}

%because I cannot use natbib
\newcommand{\citep}{\cite}
\newcommand{\citet}[1]{\citeauthor{#1}, \citeyear{#1}}

\nocopyright

\begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Compressed Conditional Mean Embeddings for Model-Based Reinforcement Learning - Supplementary Material}
\author{Guy Lever \\ University College London \\ London, UK \\ g.lever@cs.ucl.ac.uk \And John Shawe-Taylor \\ University College London \\ London, UK \\ j.shawe-taylor@cs.ucl.ac.uk \And Ronnie Stafford \\ University College London \\ London, UK \\ r.stafford.12@ucl.ac.uk \And Csaba Szepesv\'{a}ri \\ University of Alberta \\ Edmonton, Canada \\ szepesva@cs.ualberta.ca  }
\maketitle

\section*{Supplementary Material}


The following sections are supplementary to the published AAAI version of this paper.

\section{Additional Details of the Feature Selection Process}\label{MatchingPursuit}

\subsection{Sparse-Greedy Regression with Vector-Valued Matching Pursuit}

Our algorithm uses, as a subcomponent, a vector-valued version of the matching pursuit algorithm \citep{\MallatMatchingPursuit}. The use of matching pursuit for vector-valued regression is not new \citep{\LeverPoliciesInRKHS} but we provide details for completeness. This adaptation can handle targets in general vector spaces $\cV$. Since the algorithm only computes inner products between vectors in the target space $\cV$ it can be kernelized, i.e. we can learn an RKHS-valued function. This is a straightforward extension of the scalar case, we derive the method here for clarity.

Suppose we wish to regress a vector-valued function
\begin{align}
f^*:\cX\to\cV, \nn
\end{align}
given a data sample $\cD = \{x_i,v_i\}_{i=1}^m$ where $v_i = f^*(x_i)+\epsilon$ where $\epsilon$ is zero-mean noise, $f^*(x_i) = \E[V_i|x_i]$. Suppose we are given a \emph{dictionary} $\cG = \{g_1,...,g_n\}$, where $g_i:\cX\to\R$, of candidate real-valued functions, and we aim to find an estimate $\hat f$ for $f^*$ of the form,
\begin{align}
\hat f = \sum_{i=1}^D w^i \hat g_i \nn
\end{align}
where $\cB_D = \{ \hat g_i \}_{i=1}^D \subseteq \cG$ is called the basis and $w^i\in\cV$. When $\cV=\R$, matching pursuit \citep{\MallatMatchingPursuit} can be used to incrementally build the basis, and we now detail the extension to the vector-valued output case. We build the basis incrementally and for each basis $\cB_j$ we form an estimate $\hat f^j = \sum_{i=1}^j w^i \hat g_i$. We begin with the empty basis $\cB_0$  and add new basis elements $\hat g_{j+1}$ to greedily optimize the objective. For each estimate we define the residue $\br^j$,
\begin{align}
r^j_i = v_i - \hat f_j(x_i) \in\cV , \nn
\end{align}
and pick the $g\in\cD$ which minimizes the next residue when added to the current estimate,
\begin{align}
g_{j+1} &= \argmin_{g\in\cD}\min_{w \in\cV} \sum_{i=1}^m ||v_i - ( (\hat f_j + w g) (x_i)  )||_{\cV}^2 \nn\\
&=\argmin_{g\in\cD}\min_{w\in\cV} \sum_{i=1}^m ||r^j_i -  w g(x_i)  ||_{\cV}^2. \nn
\end{align}
Since $\nabla_{w}  \sum_{i=1}^m ||r^j_i -  w g(x_i)  ||_\cV^2  = 0$ at the minimum we have,
\begin{align}
0 &= \sum_{i=1}^m  \nabla_w \left(  \lang  g(x_i)w,g(x_i)w  \rang_{\cV} -2\lang g(x_i) w, r^j_i \rang_\cV \right)  \nn\\
&=  \sum_{i=1}^m 2w g(x_i)^2 -2 g(x_i)  r^j_i  \nn\\
w^{j+1} &=  \left(\sum_{i=1}^m  g(x_i)  r^j_i \right) /\left( \sum_{i=1}^m  g(x_i)^2 \right) \in\cV \nn
\end{align}
Then,
\begin{align}
&\sum_{i=1}^m ||r^j_i -  w^{j+1} g(x_i)  ||_{\cV}^2 \nn\\
&=  \sum_{i=1}^m ||r^j_i||_\cV^2 - 2\sum_{i=1}^m g(x_i)\lang  r^j_i,w^{\rm min} \rang_\cV \nn\\
&\quad + ||w^{\rm min}||_\cV^2 \sum_{i=1}^m  g(x_i)^2 \nn\\
&= \sum_{i=1}^m ||r^j_i||_\cV^2 - \frac{2\sum_{i=1}^m g(x_i)\lang  r^j_i, \sum_{k=1}^m  g(x_k)  r^j_k   \rang_\cV}{ \sum_{k=1}^m  g(x_k)^2} \\
&\quad + \frac{ ||\sum_{k=1}^m  g(x_k)  r^j_k  ||_\cV^2 }{ \sum_{i=1}^m  g(x_i)^2 }\nn\\
&= \sum_{i=1}^m ||r^j_i||_\cV^2 - \frac{ ||\sum_{i=1}^m  g(x_i)  r^j_i  ||_\cV^2 }{ \sum_{i=1}^m  g(x_i)^2 }\nn
\end{align}
Thus $\hat g_{j+1} = \argmax_{g\in\cG}  \frac{ ||\sum_{i=1}^m  g(x_i)  r^j_i  ||_\cV^2 }{ \sum_{i=1}^m  g(x_i)^2 }$. Thus at each iteration of matching pursuit we must evaluate $\frac{ ||\sum_{i=1}^m  g(x_i)  r^j_i  ||_\cV^2 }{ \sum_{i=1}^m  g(x_i)^2 }$ for a selection of $k$ dictionary elements (not necessarily all). We have,
\begin{align}
 ||\sum_{i=1}^m  g(x_i)  r^j_i  ||_\cV^2  &=  ||\sum_{i=1}^m  g(x_i) (\hat f^j(x_i) - v_i)  ||_\cV^2 \nn
\end{align}
For each dictionary element $g$ this can be computed in $O(mj+md+jd)$ where $d={\rm dim}(\cV)$, and so $O(k(mj+md+jd))$ over $k$ dictionary elements.

It is sometimes useful, at iteration $j$ to ``backfit'' all the weights $\{w^i\}_{i=1}^j$ by replacing them with the least squares solution: i.e. matching pursuit is used to find the basis but the weights are finally optimized using least squares. Alternatively this can be performed end of the process or several times throughout.

In order to find a compact representation we can also use matching pursuit adaptively by setting a tolerance $\delta$ such that the algorithm terminates when it fails to reduce the residue by more than $\delta$. Thus the method will only add features if they significantly reduces the objective.

The output of vector valued matching pursuit is a collection of weights $\{w^i\}_{i=1}^j$ and features $\{ \hat g_i(\cdot) \}_{i=1}^j$ such that $f^* \approx \sum_{i=1}^j w^i \hat g_i$. 


\subsubsection{Fast Feature Selection For RKHS-Valued Matching Pursuit}

Performing matching pursuit to regress a function $f:\cS\times\cA\to\cF_L$ where $\cF_L$ is an RKHS whose feature map $\phi(s)=L(s,\cdot)$ is high- or even infinite-dimensional can become expensive when the number of data points becomes large. This is because the expansion of each target residue for matching pursuit will have an expansion in the RKHS $\cF_L$ in the number of data points, and so computing a single inner product scales with the size of the data, and feature selection by this method would be the bottleneck in our Algorithm. To solve this problem we map the target data for matching pursuit, i.e. the residues $\cR(s_i,a_i,s_i')\in\cF_L$ for $(s_i,a_i,s'_i)\in\cD$, into a lower dimensional subspace of $\cF_L$ using an incomplete Cholesky decomposition of the kernel matrix $\bL$ \citep{\ShaweTaylorBook}. This provides an approximation $\bL \approx \bR^\top \bR$ of the $n\times n$ matrix $\bL$ where $\bR$ is a $p\times n$ matrix $\bR = ( \br_1,...,\br_n)^\top$, where $p\ll n$ can be chosen or chosen adaptively using a tolerance parameter. This approximation is often excellent \citep{\BachLowRank} and captures the inner products in $\cF_L$ since $\br_i^\top \br_j \approx L_{ij} = \lang \phi(s'_i), \phi(s'_j)\rang_L$, thus we can ``project'' the high-dimensional feature vectors $\phi(s'_i)$ to $\R^p$ via $\phi(s'_i)\mapsto \br_i$, approximately preserving inner products. We then have the following approximation for the loss function,
\begin{align}
\hat\loss_{\lambda}(\bW) &= \frac{1}{n}\sum_{i=1}^n ||\sum_{j=1}^n \psi(s_i,a_i)^\top \bw_j\phi(s_j) - \phi(s_i) ||^2_\cF \nn\\
&\approx \frac{1}{n}\sum_{i=1}^n ||\sum_{j=1}^n \psi(s_i,a_i)^\top \bw_j\br^\top_j - \br^\top _i ||^2
\end{align}
Thus if we define the following ``projection'' of the model residues
\begin{align}
\bq_i&:= \br^\top _i - \sum_{j= 1}^{n_k} \psi^k(s_i,a_i)^\top  \bw^j_{k}\br^\top _j \in \R^p, \label{ProjectedResidues}
\end{align}
and we perform vector-valued matching pursuit using the approximate, low dimensional residue data $\{(s_i,a_i),\bq_i\}_{i=1}^n$ in $p$-dimensional Euclidean space we will find find a feature $\psi^{\rm new}(\cdot)$ and a weight $\bbb^{\rm new} = \sum_{j=1}^n w_j^{\rm new} \br_j^\top$ such that
\begin{align}
\sum_{i=1}^n ||\psi^{\rm new}(s_i,a_i)^\top \bbb^{\rm new}+ \sum_{j=1}^n \psi(s_i,a_i)^\top \bw_j\br^\top_j - \br^\top _i ||^2 \nn
\end{align}
is minimized w.r.t $\psi^{\rm new}(\cdot)\in\cG$ and $\bbb^{\rm new}\in\R^p$. But then $\psi^{\rm new}(\cdot)$ and a $b^{\rm new} := \sum_{j=1}^n w_j^{\rm new} \phi(s'_j)$ are (approximately) optimally greedy for $\hat\loss_{\lambda}(\bW)$ in the sense that
\begin{align}
\sum_{i=1}^n\hspace{-0.6mm} || \psi^{\rm new}(s_i,a_i)^\top b^{\rm new} \hspace{-0.6mm} +\hspace{-0.6mm} \sum_{j=1}^n\hspace{-0.6mm} \psi(s_i,a_i)^\top \bw_j\phi(s_j)\hspace{-0.6mm} - \hspace{-0.6mm}\phi(s_i) ||^2_\cF \nn
\end{align}
is minimized w.r.t. $\psi^{\rm new}(\cdot)\in\cG$ and $b^{\rm new}\in\cF$. i.e. performing matching pursuit on the low dimensional residues is an equivalent problem up to approximation error of the Cholesky decomposition and the same features will be returned if this decomposition is accurate.  In this way $d_{\rm new}$ new features can be added in time $O(d_{\rm new} p|\cG||\cD|)$, where $p$ is the rank of the incomplete Cholesky decomposition. In our experiments $p$ was set to $p=200$ by computational budget and the kernel approximation was found to be almost perfect.


%\clearpage
\begin{small}
%\fontsize{9.5pt}{10.5pt} \selectfont
\bibliographystyle{aaai}
\bibliography{cme4mdp}
%\bibliographystyle{apalike}
%\bibliographystyle{abbrv}
\end{small}
%\fontsize{10pt}{12pt} \selectfont

\end{document}
